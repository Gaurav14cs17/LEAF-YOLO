# LEAF-YOLO Ultra - Sub-1MB Ultra-Lightweight Model Configuration
# Designed for maximum efficiency with minimal parameters (<800K)
# Uses advanced efficiency techniques: Ghost modules, Depthwise separable convs, Inverted residuals

# Model scaling parameters (Ultra-lightweight)
depth_multiple: 0.33
width_multiple: 0.125  # Even smaller than nano for sub-1MB
max_channels: 512      # Reduced max channels

# Advanced efficiency settings
use_ghost_modules: true
use_depthwise_separable: true
use_inverted_residuals: true
use_micro_attention: true
channel_divisor: 8

# Architecture definition
# [from, number, module, args]
backbone:
  # Stem - Ultra-efficient focus
  - [-1, 1, UltraFocus, [64, 3]]  # 0-P1/2  (320x320 -> 160x160, 12 channels)
  
  # Stage 1 - Minimal feature extraction
  - [-1, 1, DWConvUltra, [32, 3, 2]]  # 1-P2/4  (160x160 -> 80x80, 16 channels)
  - [-1, 1, C3Ultra, [32, 1]]          # 2      (80x80, 16 channels)
  
  # Stage 2 - Efficient processing
  - [-1, 1, DWConvUltra, [64, 3, 2]]  # 3-P3/8  (80x80 -> 40x40, 32 channels)
  - [-1, 2, C3Ultra, [64, 2]]          # 4      (40x40, 32 channels)
  
  # Stage 3 - Feature refinement
  - [-1, 1, DWConvUltra, [128, 3, 2]] # 5-P4/16 (40x40 -> 20x20, 64 channels)
  - [-1, 3, C3Ultra, [128, 3]]         # 6      (20x20, 64 channels)
  
  # Stage 4 - High-level features
  - [-1, 1, DWConvUltra, [256, 3, 2]] # 7-P5/32 (20x20 -> 10x10, 128 channels)
  - [-1, 1, C3Ultra, [256, 1]]         # 8      (10x10, 128 channels)
  - [-1, 1, SPPFUltra, [256, 5]]       # 9      (10x10, 128 channels)

# Head
head:
  # FPN-style head with ultra-lightweight convolutions
  - [-1, 1, DWConvUltra, [128, 1, 1]]                    # 10 (128 channels)
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]           # 11 (20x20)
  - [[-1, 6], 1, Concat, [1]]                            # 12 cat backbone P4
  - [-1, 1, C3Ultra, [128, 1, False]]                    # 13 (20x20, 64 channels)
  
  - [-1, 1, DWConvUltra, [64, 1, 1]]                     # 14 (64 channels)
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]           # 15 (40x40)
  - [[-1, 4], 1, Concat, [1]]                            # 16 cat backbone P3
  - [-1, 1, C3Ultra, [64, 1, False]]                     # 17 (40x40, 32 channels)
  
  # Detection outputs with shared convolutions
  - [-1, 1, DWConvUltra, [64, 3, 2]]                     # 18 (40x40 -> 20x20)
  - [[-1, 14], 1, Concat, [1]]                           # 19 cat head P4
  - [-1, 1, C3Ultra, [128, 1, False]]                    # 20 (20x20, 64 channels)
  
  - [-1, 1, DWConvUltra, [128, 3, 2]]                    # 21 (20x20 -> 10x10)
  - [[-1, 10], 1, Concat, [1]]                           # 22 cat head P5
  - [-1, 1, C3Ultra, [256, 1, False]]                    # 23 (10x10, 128 channels)
  
  # Ultra-lightweight detection head
  - [[17, 20, 23], 1, UltraDetect, [nc]]                 # 24 Detect(P3, P4, P5)

# Training optimization for ultra-lightweight model
training:
  # Use knowledge distillation from larger model
  use_distillation: true
  teacher_model: "leafyolo_s"
  distillation_alpha: 0.7
  distillation_temperature: 4.0
  
  # Advanced optimization
  use_ema: true
  ema_decay: 0.9999
  
  # Label smoothing for better generalization
  label_smoothing: 0.1
  
  # Progressive resizing
  progressive_resizing: true
  min_img_size: 320
  max_img_size: 640

# Inference optimizations
inference:
  # Optimizations for deployment
  fuse_conv_bn: true
  optimize_for_mobile: true
  quantization_ready: true
  
  # Input size optimization
  preferred_img_size: 416  # Smaller default for efficiency
  
# Expected performance targets
performance:
  target_parameters: "0.8M"    # Sub-1MB target
  target_flops: "1.2G"        # Very low computational cost
  expected_map50: "32-35%"    # Decent accuracy for size
  expected_speed: "12-15ms"   # Very fast inference
